---
title: "Download and Merge Data"
subtitle: "SKILLS: Data Preparation and Workflow Management - Group 9"
author:
  - "Rabino Tommaso"
  - "Franceschini Emanuele"
  - "Magalotti Bianca"
  - "Tan Colin"
  - "Benmrit Akram"
date: "\\textit{20 October 2024}"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
header-includes:
  - "\\usepackage[english]{babel}"  # Language
  - "\\usepackage[T1]{fontenc}"  # Font encoding
  - "\\usepackage{mathptmx}"  # Times New Roman font for text
  - "\\usepackage{helvet}"  # Arial-like font for sans-serif
  - "\\usepackage{setspace}"  # Line spacing
  - "\\onehalfspacing"  # 1.5 line spacing
  - "\\usepackage{fancyhdr}"  # Header and footer customization
  - "\\usepackage{titlesec}"  # Section titles formatting
  - "\\usepackage{abstract}"  # Abstract formatting
  - "\\usepackage{caption}"  # Captions customization
  - "\\usepackage{graphicx}"  # Graphics
  - "\\usepackage{amsmath}"  # Math equations
  - "\\usepackage{amssymb}"  # Math symbols
  - "\\usepackage{natbib}"  # Citation style (change as needed)
  - "\\bibliographystyle{apalike}"  # Bibliography style (change as needed)
  - "\\usepackage{hyperref}"  # Hyperlinks and URLs
  - "\\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}"
  - "\\usepackage{appendix}"  # Appendix formatting
  - "\\usepackage{enumerate}"  # Enumerate environment
  - "\\pagestyle{fancy}"  # Custom page style
  - "\\fancyhf{}"
  - "\\renewcommand{\\headrulewidth}{0pt}"
  - "\\renewcommand{\\footrulewidth}{0pt}"
  - "\\fancyhead[R]{\\thepage}"  # Page number in the header
  - "\\fancypagestyle{plain}{\\fancyhf{}\\renewcommand{\\headrulewidth}{0pt}}"
  - "\\lhead{\\small{A.A. 2023/2024-Courses: SKILLS: Data Preparation and Workflow Management}}"
  - "\\usepackage{multicol}"  # For two columns
geometry: "left=2.5cm, right=1.5cm, top=2.5cm, bottom=2.5cm"  # Adjust margins
---

```{r, include=FALSE}
#Load Required Packages
rm(list=ls())
library(readr)
```

\section{Download Dataset}
The purpose of this code is to automate the download and extraction of \textit{Airbnb} listing data from all available datasets for Italian cities on Airbnb.

\textbf{Preliminary Settings: } Before actually download the datasets, we need to accomplish some preliminary steps:
\begin{itemize}
  \item The url variable is a vector that contains URLs pointing to ZIP files hosted online. Each URL corresponds to Airbnb listing data for a specific city or region in Italy.
  \item The names variable is another vector that provides user-defined names for each URL. These names will be used to label the downloaded data frames, making them easier to work with.
  \item The data\_dir variable specifies the target directory where the ZIP files will be downloaded and extracted.
\end{itemize}

```{r}
# URLs for downloading ZIP files
url <- c(
  "http://data.insideairbnb.com/italy/lombardia/bergamo/2023-06-30/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/emilia-romagna/bologna/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/toscana/florence/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/lombardy/milan/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/campania/naples/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/puglia/puglia/2023-06-30/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/lazio/rome/2023-06-10/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/sicilia/sicily/2023-06-29/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/trentino-alto-adige-s%C3%BCdtirol/trentino/2023-06-30/data/listings.csv.gz"
)

# Names for each URL
names <- c("listings_bergamo","listings_bologna", "listings_florence", "listings_milan", "listings_naples", "listings_puglia", "listings_rome", "listings_sicily", "listings_trentino")

# Define the target directory for extraction
data_dir <- "../../data/"
```

\textbf{Download and Extraction: }
In the following code block, we have implemented an automated process to download and extract data from various Airbnb listings datasets for Italian cities. The code loops through the previously created vector of URLs, checks if the corresponding CSV file already exists locally, and either downloads the file or skips the download if it already exists. After downloading, it extracts and loads the data into separate datasets, making them ready for further analysis.

```{r}
#Avoid too long downloads to be shut down
options(timeout = max(1000, getOption("timeout")))

# Loop through the URLs and extract/load the CSV files into separate datasets
for (i in 1:length(url)) {
  target_file <- paste0(data_dir, names[i], ".csv.gz")

  # Check if the CSV file already exists
  if (!file.exists(target_file)) {
    download.file(url[i], target_file, mode = "wb")
  } else {
    print(paste("ZIP file", names[i], "already exists. Skipping download."))
  }

  # Extract and load the CSV file
  gz_file <- gzfile(target_file, "rt")
  dataset_name <- paste("data_", names[i], sep = "")  # Create a unique dataset name
  assign(dataset_name, read.csv(gz_file))  # Load data into the dataset_name
  close(gz_file)  # Close the GZ file
  print(paste("Loaded", names[i]))
}
```

```{r, include=FALSE}
rm(url, target_file, names, i, gz_file, dataset_name)
```



\section{Merge Dataset}
In the following code section, we perform data merging and saving tasks to prepare the Airbnb listings dataset for our analysis. The code combines multiple datasets corresponding to different Italian cities into a single merged dataset. It also handles the removal of the source datasets to free up memory space. Finally, the merged dataset is saved for future use.

```{r}
# List of dataset names
dataset_names <- c("data_listings_milan", "data_listings_bergamo", "data_listings_bologna", "data_listings_florence", "data_listings_naples", "data_listings_puglia", "data_listings_rome", "data_listings_sicily", "data_listings_trentino")

# Initialize the merged dataset with the first dataset
data <- get(dataset_names[1])

# Loop through the remaining datasets and merge them
for (name in dataset_names[-1]) {
  data <- rbind(data, get(name))
}

# Deleting "source" datasets
for (name in dataset_names) {
  rm(list = name)
}

# Save the data as a CSV file
write.csv(data, file = "../../gen/data-preparation/input/merged_data.csv", fileEncoding = "UTF-8",row.names=FALSE )

```

```{r, include=FALSE}
rm(dataset_names, name, data)
```



\section{Delete Source ZIP Files}
In this section, we address the cleanup process by removing the source ZIP files that were downloaded earlier. These ZIP files contained individual Airbnb listings data for various Italian cities. Once the datasets have been merged and the relevant data extracted, these ZIP files are no longer needed and can be safely deleted to free up storage space. The code below iterates through the list of ZIP file names and deletes them if they exist. This step ensures that we maintain an organized and clutter-free working environment.

```{r}
# List of ZIP file names
zip_file_names <- c("listings_milan.csv.gz", "listings_bergamo.csv.gz", "listings_bologna.csv.gz", "listings_florence.csv.gz", "listings_naples.csv.gz", "listings_puglia.csv.gz", "listings_rome.csv.gz", "listings_sicily.csv.gz", "listings_trentino.csv.gz")

# Loop through the ZIP file names and delete them
for (zip_name in zip_file_names) {
  zip_path <- file.path(data_dir, zip_name)
  if (file.exists(zip_path)) {
    file.remove(zip_path)
    print(paste("Deleted", zip_name))
  } else {
    print(paste("ZIP file", zip_name, "does not exist. Skipping deletion."))
  }
}
```

```{r, include=FALSE}
rm(data_dir, zip_file_names, zip_name, zip_path)
```
