---
title: "Download and Merge Data"
subtitle: "SKILLS: Data Preparation and Workflow Management - Group 9"
author:
  - "Rabino Tommaso"
  - "Franceschini Emanuele"
  - "Magalotti Bianca"
  - "Tan Colin"
  - "Benmrit Akram"
date: "\\textit{20 October 2024}"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
header-includes:
  - "\\usepackage[english]{babel}"  # Language
  - "\\usepackage[T1]{fontenc}"  # Font encoding
  - "\\usepackage{mathptmx}"  # Times New Roman font for text
  - "\\usepackage{helvet}"  # Arial-like font for sans-serif
  - "\\usepackage{setspace}"  # Line spacing
  - "\\onehalfspacing"  # 1.5 line spacing
  - "\\usepackage{fancyhdr}"  # Header and footer customization
  - "\\usepackage{titlesec}"  # Section titles formatting
  - "\\usepackage{abstract}"  # Abstract formatting
  - "\\usepackage{caption}"  # Captions customization
  - "\\usepackage{graphicx}"  # Graphics
  - "\\usepackage{amsmath}"  # Math equations
  - "\\usepackage{amssymb}"  # Math symbols
  - "\\usepackage{natbib}"  # Citation style (change as needed)
  - "\\bibliographystyle{apalike}"  # Bibliography style (change as needed)
  - "\\usepackage{hyperref}"  # Hyperlinks and URLs
  - "\\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}"
  - "\\usepackage{appendix}"  # Appendix formatting
  - "\\usepackage{enumerate}"  # Enumerate environment
  - "\\pagestyle{fancy}"  # Custom page style
  - "\\fancyhf{}"
  - "\\renewcommand{\\headrulewidth}{0pt}"
  - "\\renewcommand{\\footrulewidth}{0pt}"
  - "\\fancyhead[R]{\\thepage}"  # Page number in the header
  - "\\fancypagestyle{plain}{\\fancyhf{}\\renewcommand{\\headrulewidth}{0pt}}"
  - "\\lhead{\\small{A.A. 2023/2024-Courses: SKILLS: Data Preparation and Workflow Management}}"
  - "\\usepackage{multicol}"  # For two columns
geometry: "left=2.5cm, right=1.5cm, top=2.5cm, bottom=2.5cm"  # Adjust margins
---

```{r, include=FALSE}
#Load Required Packages
rm(list=ls())
library(readr)
```

\section{Download Dataset}
The purpose of this code is to automate the download and extraction of \textit{Airbnb} listing data from various datasets representing Italian cities and regions on Airbnb. This automation streamlines the process of acquiring data for research and analysis.

\textbf{Dataset Selection:} The initial step in our analysis involved the critical task of dataset selection. To ensure a comprehensive study, we opted to acquire and utilize all available Airbnb listings datasets for cities and regions in Italy. These datasets were sourced from \href{http://insideairbnb.com/get-the-data/}{Inside Airbnb}, the official Airbnb data provider. The choice of this dataset is driven by its relevance to our research objectives. We opt for Italian cities because they offer a diverse set of locations, each with its unique property types, features and pricing trends. By analyzing this dataset, we aim to gain valuable insights into the dynamics of Airbnb listings in Italy, helping us draw conclusions for our research.

\textbf{Preliminary Settings:} Before proceeding with the dataset download and extraction, we need to complete some preliminary steps:
\begin{itemize}
  \item The \texttt{url} object is a vector containing URLs pointing to ZIP files hosted by Inside Airbnb. Each URL corresponds to Airbnb listing data for a specific city or region in Italy.
  \item The \texttt{names} object is another vector providing user-defined names for each URL. These names will be used to label the downloaded data frames, making them easier to work with.
  \item The \texttt{data\_dir} object specifies the target directory where the ZIP files will be downloaded and extracted.
\end{itemize}

These preliminary settings ensure that we have organized access to the datasets we need for our research. With this foundation, we can proceed to acquire, explore and analyze the data.

```{r}
# URLs for downloading ZIP files
url <- c(
  "http://data.insideairbnb.com/italy/lombardia/bergamo/2023-06-30/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/emilia-romagna/bologna/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/toscana/florence/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/lombardy/milan/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/campania/naples/2023-06-21/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/puglia/puglia/2023-06-30/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/lazio/rome/2023-06-10/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/sicilia/sicily/2023-06-29/data/listings.csv.gz",
  "http://data.insideairbnb.com/italy/trentino-alto-adige-s%C3%BCdtirol/trentino/2023-06-30/data/listings.csv.gz"
)

# Names for each URL
names <- c("listings_bergamo","listings_bologna", "listings_florence", "listings_milan", "listings_naples", "listings_puglia", "listings_rome", "listings_sicily", "listings_trentino")

# Define the target directory for extraction
data_dir <- "../../data/"
```


\textbf{Download and Extraction:}
In the subsequent code section, we've implemented an automated procedure for downloading and extracting data from all Italian Airbnb listings datasets hosted by Inside Airbnb. This code operates by iterating through the previously defined list of URLs, conducting checks to ascertain whether the corresponding ZIP file already resides locally. Based on the outcome of this check, the code either initiates a download or bypasses it. Following successful downloads, the code proceeds to extract the CSV file from the downloaded ZIP file, and organize the data into distinct datasets, rendering them prepared for subsequent analysis.

```{r}
#Avoid too long downloads to be shut down
options(timeout = max(1000, getOption("timeout")))

# Loop through the URLs and extract/load the CSV files into separate datasets
for (i in 1:length(url)) {
  target_file <- paste0(data_dir, names[i], ".csv.gz")

  # Check if the CSV file already exists
  if (!file.exists(target_file)) {
    download.file(url[i], target_file, mode = "wb")
  } else {
    print(paste("ZIP file", names[i], "already exists. Skipping download."))
  }

  # Extract and load the CSV file
  gz_file <- gzfile(target_file, "rt")
  dataset_name <- paste("data_", names[i], sep = "")  # Create a unique dataset name
  assign(dataset_name, read.csv(gz_file))  # Load data into the dataset_name
  close(gz_file)  # Close the GZ file
  print(paste("Loaded", names[i]))
}
```

```{r, include=FALSE}
rm(url, target_file, names, i, gz_file, dataset_name)
```



\section{Data Merging and Storage}
In the subsequent code segment, we execute a data merging task to prepare the Airbnb listings dataset for our analysis. This code combines the numerous datasets previously downloaded and extracted, uniting them into a single, consolidated dataset. Additionally, it manages the removal of the source datasets, a practice designed to liberate valuable memory resources.

Furthermore, the code ensures the lasting accessibility of the merged dataset by saving it in two distinct formats. It is preserved both as a CSV file, facilitating external examination outside the RStudio environment, and as a .RDS (R Data Serialization) object. The latter format is chosen to simplify the reloading process, ensuring seamless integration into subsequent project sections.

```{r}
# List of dataset names
dataset_names <- c("data_listings_milan", "data_listings_bergamo", "data_listings_bologna", "data_listings_florence", "data_listings_naples", "data_listings_puglia", "data_listings_rome", "data_listings_sicily", "data_listings_trentino")

# Initialize the merged dataset with the first dataset
data <- get(dataset_names[1])

# Loop through the remaining datasets and merge them
for (name in dataset_names[-1]) {
  data <- rbind(data, get(name))
}

# Deleting "source" datasets
for (name in dataset_names) {
  rm(list = name)
}

# Save the data as a CSV file
write.csv(data, file = "../../data/dataset1/merged_data.csv", fileEncoding = "UTF-8",row.names=FALSE )

#Save the data as .Rds file
saveRDS(data, file = "../../gen/data-preparation/input/merged_data.rds")

```

```{r, include=FALSE}
rm(dataset_names, name, data)
```



\section{Delete Source ZIP Files}
In this section, we address the cleanup process by removing the source ZIP files that were downloaded earlier. These ZIP files contained individual Airbnb listings data for various Italian cities. Once the relevant data have been extracted and the datasets merged, these ZIP files are no longer needed and can be safely deleted to free up storage space. The code below iterates through the list of ZIP file names and deletes them if they exist. This step ensures that we maintain an organized and clutter-free working environment.

```{r}
# List of ZIP file names
zip_file_names <- c("listings_milan.csv.gz", "listings_bergamo.csv.gz", "listings_bologna.csv.gz", "listings_florence.csv.gz", "listings_naples.csv.gz", "listings_puglia.csv.gz", "listings_rome.csv.gz", "listings_sicily.csv.gz", "listings_trentino.csv.gz")

# Loop through the ZIP file names and delete them
for (zip_name in zip_file_names) {
  zip_path <- file.path(data_dir, zip_name)
  if (file.exists(zip_path)) {
    file.remove(zip_path)
    print(paste("Deleted", zip_name))
  } else {
    print(paste("ZIP file", zip_name, "does not exist. Skipping deletion."))
  }
}
```

```{r, include=FALSE}
rm(data_dir, zip_file_names, zip_name, zip_path)
```
