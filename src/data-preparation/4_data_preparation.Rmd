---
title: "Data Preparation before Modelling the Regression"
subtitle: "SKILLS: Data Preparation and Workflow Management - Group 9"
author:
  - "Rabino Tommaso"
  - "Franceschini Emanuele"
  - "Magalotti Bianca"
  - "Tan Colin"
  - "Benmrit Akram"
date: "\ {20 October 2024}"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
header-includes:
  - "\\usepackage[english]{babel}"  # Language
  - "\\usepackage[T1]{fontenc}"  # Font encoding
  - "\\usepackage{mathptmx}"  # Times New Roman font for text
  - "\\usepackage{helvet}"  # Arial-like font for sans-serif
  - "\\usepackage{setspace}"  # Line spacing
  - "\\onehalfspacing"  # 1.5 line spacing
  - "\\usepackage{fancyhdr}"  # Header and footer customization
  - "\\usepackage{titlesec}"  # Section titles formatting
  - "\\usepackage{abstract}"  # Abstract formatting
  - "\\usepackage{caption}"  # Captions customization
  - "\\usepackage{graphicx}"  # Graphics
  - "\\usepackage{amsmath}"  # Math equations
  - "\\usepackage{amssymb}"  # Math symbols
  - "\\usepackage{natbib}"  # Citation style (change as needed)
  - "\\bibliographystyle{apalike}"  # Bibliography style (change as needed)
  - "\\usepackage{hyperref}"  # Hyperlinks and URLs
  - "\\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}"
  - "\\usepackage{appendix}"  # Appendix formatting
  - "\\usepackage{enumerate}"  # Enumerate environment
  - "\\pagestyle{fancy}"  # Custom page style
  - "\\fancyhf{}"
  - "\\renewcommand{\\headrulewidth}{0pt}"
  - "\\renewcommand{\\footrulewidth}{0pt}"
  - "\\fancyhead[R]{\\thepage}"  # Page number in the header
  - "\\fancypagestyle{plain}{\\fancyhf{}\\renewcommand{\\headrulewidth}{0pt}}"
  - "\\lhead{\\small{A.A. 2023/2024-Courses: SKILLS: Data Preparation and Workflow Management}}"
  - "\\usepackage{multicol}"  # For two columns
geometry: "left=2.5cm, right=1.5cm, top=2.5cm, bottom=2.5cm"  # Adjust margins
---

```{r, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE,
                      size = "small",
                      out.width = "100%",
                      message = FALSE,
                      warning = FALSE,
                      error = FALSE)
```


```{r, include = FALSE}
###############################################################
#############             PACKAGES               ##############
###############################################################
#GENERAL PACKAGES:
library(tidyverse) #A "Package of Packages" for Data manipulation and visualization (includes magrittr, lubridate, purrr, tidyr, etc.).
library(dplyr) #Data frame manipulations (select, slice, etc.
library(jsonlite) #For Amenities Columns Creation

#PLOT PACKAGES:
library(ggplot2) #Building fancy plots.
library(ggthemes) #Themes for ggplots (e.g. "solarized").
library(ggcorrplot) #For correlograms
library(scales) #Scaling and formatting ggplots (e.g. scale_fill_gradient()).
library(gt) #Latex tables
library(moments) #Measuring the skewness of our dependent variable (price).
library(caret)
```


```{r, echo=FALSE}
# Load the R object
clean_data <- readRDS("../../gen/data-preparation/input/clean_data.rds")
```

\newpage
\section{Introduction}
In this R file we will prepare the dataset for the subsequent modelling of the regression. Indeed, although through the previous analyses we have managed to avoid many of the problems typical of regression models, we still have a few steps left to perform before constructing our regression model. All these data preparation operations will be performed in the following sections. 


\section{Correlations to handle multicollinearity}
In the present section, the drop of highly correlated predictor variables was undertaken, since this operation is useful for several reasons:

\begin{itemize}

  \item \textit{Reducing Multicollinearity}: When two or more predictor variables are highly correlated with each other, they provide redundant information to the model. This multicollinearity can cause problems in estimating the regression coefficients, as it becomes difficult to distinguish the independent effects of each variable. 
  \item \textit{Improving Model Interpretability}: A model with fewer predictors is easier to interpret and explain. Dropping highly correlated predictors can help to simplify the model and make it easier to explain.
  \item \textit{Avoiding Overfitting}: A model with too many predictors can lead to overfitting, where the model is too closely fitted to the training data and may not generalize well to new data.

\end{itemize}


\textbf{Numeric Variables Correlation}: Here is a correlogram displaying correlation values for all numeric variables and \texttt{price}.
```{r, include=FALSE}
corr_data <- clean_data %>% dplyr::select_if(is.numeric)

corr_matrix <- round(cor(corr_data),2)

# Define custom color palette
my_colors <- c("#E6E6E6", "#FFFFFF", "#CCCCCC")

# Restyled correlogram plot
plot <- ggcorrplot(corr_matrix,
           hc.order = FALSE,
           method = "square",
           type = "lower",
           outline.col = "white",
           colors = my_colors,
           lab = TRUE, 
           lab_size = 2.5,
           digits = 2,
           tl.cex = 8,
           title = "Numeric Variables Correlogram") +
  theme(legend.position = "none", 
        plot.title = element_text(lineheight = 2, face = "bold", vjust = 1, hjust = 0.5, color = "#333333"),
        plot.subtitle = element_text(color = "#666666", vjust = 1, hjust = 0.5),
        axis.title = element_text(color = "#666666"), 
        axis.text = element_text(color = "#666666"))

```

```{r, echo=FALSE}
plot
```

\textbf{Highly Correlated Variable Pairs}: To improve clarity, we created a table displaying, in descending order, the pairs of numeric variables with a mutual correlation higher than 0.75. This table helps in identifying highly correlated variables for potential removal to avoid multicollinearity in regression analysis.
```{r, include=FALSE}
# Convert correlation matrix into a dataframe
corr_matrix_df <- data.frame(corr_matrix)

# Add a new column for variable names
corr_matrix_df$Variable <- row.names(corr_matrix_df)

# Define the threshold for correlation
threshold <- 0.75

# Create a data frame to store pairs of highly correlated variables
high_corr_pairs <- data.frame(Variable1 = character(0), Variable2 = character(0), Correlation = numeric(0))

# Loop through the rows and columns of the correlation matrix
for (i in 1:nrow(corr_matrix)) {
  for (j in 1:ncol(corr_matrix)) {
    if (!is.na(corr_matrix[i, j]) && i < j && abs(corr_matrix[i, j]) > threshold) {
      high_corr_pairs <- rbind(high_corr_pairs, 
                               data.frame(Variable1 = rownames(corr_matrix)[i],
                                          Variable2 = colnames(corr_matrix)[j],
                                          Correlation = corr_matrix[i, j]))
    }
  }
}

# Arrange the pairs by correlation in descending order
high_corr_pairs <- high_corr_pairs %>% arrange(desc(high_corr_pairs$Correlation))

table_high_correlation_pairs <- high_corr_pairs %>%
  gt() %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_header(
    title = "Highly Correlated Variable Pairs",
    subtitle = paste("Pairs with correlation > ", threshold),
  )


```

```{r, echo=FALSE}
table_high_correlation_pairs
```


\textbf{Variable Remotion}: The code snippet below is utilized to eliminate one variable from the dataset for each pair of highly correlated variables previously identified. The criterion for removal is to discard the variable with the lowest correlation with \texttt{price}. Between the highly correlated variables, we decided to retain only beds, accommodates, and bedroom, since their very high correlation with \texttt{price}.
```{r}
clean_data <- clean_data %>% dplyr::select(-host_total_listings_count, -availability_90, -availability_30, -review_scores_value, -review_scores_accuracy, -reviews_per_month, -review_scores_checkin)
```


\section{Dealing with skewness of price}
\textbf{Dependent Variable Skewness}: Now, we have to prepare our response variable for the modeling, because its skewness it's too high. Skewness is a measure of the symmetry in a distribution. A symmetrical variable will have a skewness equal to 0 (i.e. a normal distributed variable will have a skewness of 0). As a rule of thumb, a variable can be regarded as sufficiently normally distributed if its skewness is near to 0 or, at least, between -1 and 1.

```{r}
price_skew <- skewness(clean_data$price)
cat("As we can see, the skewness of our target variable (price) is too high: ", price_skew)
```

\textbf{New Dependent Variable}: In order to fix this issue, we computed the natural logarithm \texttt{price} and use it as response variable for our regression model.
```{r}
clean_data$log_price = log(clean_data$price)
log_price_skew <- skewness(clean_data$log_price)
cat("The skewness of our new response variable (log_price) is equal to: ", log_price_skew, "--> a much better result than the previous one.")
```

\textbf{Normal Q-Q Plots}: It is also possible to measure the improvement in the skewness of our dependent variable by inspecting the two Normal Q-Q plots below, one inspecting the distribution of \texttt{price}, and the other inspecting the distribution of the newly created variable \texttt{log\_price}. Indeed, it can be seen that \texttt{log\_price} is better distributed, since the points follow the straight line drawn at a 45-degree angle from the origin.

```{r, include=FALSE}
plot <- ggplot(clean_data, aes(sample = price)) +
  stat_qq(color = "red") +
  stat_qq_line(color = "grey", size=1) +
  labs(title = "Normal Q-Q Plot of Price",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")+
  theme(legend.position = "none", 
        plot.title = element_text(size = 18, lineheight=.8, face="bold", vjust=1, hjust=0.5, colour = "black"),
        plot.subtitle = element_text(size = 12, lineheight=.8, vjust=1, hjust=0.5, colour = "black"),
        axis.title = element_text(size = 14, colour = "black"), 
        axis.text = element_text(size = 8, colour = "#666666"),
        axis.text.x = element_text(hjust = 1),
        panel.grid.major = element_line(color = "gray", size = 0.5),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white"))
  
```  

```{r, echo=FALSE}
plot
``` 

```{r, include=FALSE}
plot_skew_log_price <- ggplot(clean_data, aes(sample = log_price)) +
  stat_qq(color = "red") +
  stat_qq_line(color = "gray", size=1) +
  labs(title = "Normal Q-Q Plot of log_price",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")+
  theme(legend.position = "none", 
        plot.title = element_text(size = 18, lineheight=.8, face="bold", vjust=1, hjust=0.5, colour = "black"),
        plot.subtitle = element_text(size = 12, lineheight=.8, vjust=1, hjust=0.5, colour = "black"),
        axis.title = element_text(size = 14, colour = "black"), 
        axis.text = element_text(size = 8, colour = "#666666"),
        axis.text.x = element_text(hjust = 1),
        panel.grid.major = element_line(color = "gray", size = 0.5),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "white"))
```  

```{r, echo=FALSE}
plot_skew_log_price
``` 


As \texttt{log\_price} will be used as response variable, we removed \texttt{price} variable from the dataset. 
```{r}
clean_data <- clean_data %>% dplyr::select(-price)
```


\section{Divide the dataset based on variables 'format'}
With the following code block we are going to split the dataset into 3 parts: one for numeric variables, one for date variables, and one for factor variables. This passage is important because we will manage each of this subdataset separately in the following data preparation sections. Specifically:
\begin{itemize}
\item Numeric Variables: The numeric Variables (except for the response variable \texttt{log\_price}) will be centered and scaled to avoid error due to different scales of measurement.
\item Date and Factor Variables: Since XGBoost Regression Models can only handle numerical variables, factor and date predictors will be converted into numeric format by using two different methods.
\end{itemize}

```{r}
numeric_clean_data <- clean_data %>% dplyr::select_if(is.numeric)
date_clean_data <- clean_data %>% dplyr::select_if(is.Date)
factor_clean_data <- clean_data %>% dplyr::select_if(is.factor)

cat("There are", length(numeric_clean_data), "numeric variables,", length(date_clean_data), "date variables, and", length(factor_clean_data), "factor variables. And they are the following: ")

```

\section{Management of date predictors}
In order to convert the date variables in numeric format we created 3 different new variables, one for the days, one for the months, and one for the years for each of the 3 date variables.

```{r}
date_clean_data$host_since_day <- as.numeric(day(date_clean_data$host_since))
date_clean_data$host_since_month <- as.numeric(month(date_clean_data$host_since))
date_clean_data$host_since_year <- as.numeric(year(date_clean_data$host_since))

date_clean_data$first_review_day <- as.numeric(day(date_clean_data$first_review))
date_clean_data$first_review_month <- as.numeric(month(date_clean_data$first_review))
date_clean_data$first_review_year <- as.numeric(year(date_clean_data$first_review))

date_clean_data$last_review_day <- as.numeric(day(date_clean_data$last_review))
date_clean_data$last_review_month <- as.numeric(month(date_clean_data$last_review))
date_clean_data$last_review_year <- as.numeric(year(date_clean_data$last_review))

date_clean_data <- date_clean_data %>% dplyr::select(-host_since, -first_review, -last_review)
```


\section{Management of numeric predictors}
In order to avoid weak estimates due to the different scales of measurement used for the different numeric variables, we performed two different operations, namely centering and scaling numeric predictors.

```{r, include=FALSE}
############### NEEDED FOR THE SHINY_APP #####################################################

accommodates_mean <- mean(numeric_clean_data$accommodates)
accommodates_sd <- sd(numeric_clean_data$accommodates)
bedrooms_mean <- mean(numeric_clean_data$bedrooms)
bedrooms_sd <- sd(numeric_clean_data$bedrooms)
beds_mean <- mean(numeric_clean_data$beds)
beds_sd <- sd(numeric_clean_data$beds)
host_listings_count_mean <- mean(numeric_clean_data$host_listings_count)
host_listings_count_sd <- sd(numeric_clean_data$host_listings_count)
bathrooms_mean <- mean(numeric_clean_data$bathrooms)
bathrooms_sd <- sd(numeric_clean_data$bathrooms)

###############################################################################################
```

```{r}
y <- numeric_clean_data$log_price

numeric_clean_data <- numeric_clean_data %>%
  dplyr::select (-log_price)

preprocessed_data <- preProcess(numeric_clean_data, method=c("center", "scale"))

print(preprocessed_data)

numeric_clean_data <- predict(preprocessed_data, newdata = numeric_clean_data)
```

\section{Management of Factor Predictors}
In order to train a regression model using categorical predictors, we have to convert them in numeric format. For doing so we used a method called One-Hot Encoding, a process of converting categorical or factor variables into a set of binary (0/1) variables that can be used in a machine learning model. In this process, each category of the factor variable is represented as a separate binary variable or column.

```{r}
factor_clean_data <- as.data.frame(model.matrix(~.-1, factor_clean_data))
```

Finally, we tried to enhance the clarity and interpretability of factor binary variables. First, the code is used to rename columns from "NO", to "YES", providing a more intuitive understanding of the data. Additionally, recoding the values ensures that the data aligns with the updated column names, making it easier for analysts and data users to work with the dataset.
```{r}
# Rename columns from "NO" to "YES" in columns 105 to 186
new_col_names <- colnames(factor_clean_data)[105:186]
new_col_names <- gsub("NO", "YES", new_col_names)
colnames(factor_clean_data)[105:186] <- new_col_names

# Replace 0 with 1 and 1 with 0 in columns 105 to 186
factor_clean_data[, 105:186] <- 1 - factor_clean_data[, 105:186]

```


\section{Combine the sub-datasets}
Then, we combined the 3 sub-datasets (i.e. \texttt{factor\_clean\_data}, \texttt{date\_clean\_data}, and \texttt{numeric\_clean\_data} that now contain only numeric variables.

```{r}
clean_data <- cbind(numeric_clean_data, date_clean_data, factor_clean_data)

cat("Now, our dataset has ", nrow(clean_data), "observations and ", ncol(clean_data), "variables")
```

\section{Variables re-naming}
After the One-Hot Encoding operation, in order to have a clearer dataset, we decided to replace all spaces with underscores within the variable names.  

```{r}
# Remove non-alphanumeric symbols (except spaces and underscores) from variable names
colnames(clean_data) <- gsub("[^[:alnum:] _]", "", colnames(clean_data))

# Replace spaces with underscores in variable names
colnames(clean_data) <- gsub(" ", "_", colnames(clean_data))
```


\section{Save relevant objects}
In conclusion, all relevant objects were preserved for future purposes.
```{r}
saveRDS(clean_data, file = "../../gen/analysis/input/regression_data.rds")
saveRDS(accommodates_mean, file = "../../gen/analysis/input/accommodates_mean.rds")
saveRDS(accommodates_sd, file = "../../gen/analysis/input/accommodates_sd.rds")
saveRDS(bedrooms_mean, file = "../../gen/analysis/input/bedrooms_mean.rds")
saveRDS(bedrooms_sd, file = "../../gen/analysis/input/bedrooms_sd.rds")
saveRDS(beds_mean, file = "../../gen/analysis/input/beds_mean.rds")
saveRDS(beds_sd, file = "../../gen/analysis/input/beds_sd.rds")
saveRDS(host_listings_count_mean, file = "../../gen/analysis/input/host_listings_count_mean.rds")
saveRDS(host_listings_count_sd, file = "../../gen/analysis/input/host_listings_count_sd.rds")
saveRDS(bathrooms_mean, file = "../../gen/analysis/input/bathrooms_mean.rds")
saveRDS(bathrooms_sd, file = "../../gen/analysis/input/bathrooms_sd.rds")
saveRDS(y, file = "../../gen/analysis/input/y.rds")
```

```{r, include=FALSE}
rm(list=ls())
```


